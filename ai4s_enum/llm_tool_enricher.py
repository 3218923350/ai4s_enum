"""
ä½¿ç”¨ LLM å°†å€™é€‰ repo è½¬æ¢ä¸ºå®Œæ•´çš„å·¥å…·å®šä¹‰
"""
import json
import time
from typing import Dict, List, Optional

from .config import EnumConfig
from .http_utils import safe_request
from .leaf_clusters import LeafCluster
from .logger import get_logger
from .units import Unit

logger = get_logger()


def llm_enrich_tools(
    cfg: EnumConfig,
    *,
    cluster: LeafCluster,
    unit: Unit,
    candidates: List[dict],
    batch_size: int = 10,
) -> List[dict]:
    """
    ä½¿ç”¨ LLM å°†å€™é€‰ repo æ‰¹é‡è½¬æ¢ä¸ºå®Œæ•´çš„å·¥å…·å®šä¹‰ã€‚
    
    è¾“å…¥ candidates æ ¼å¼ï¼ˆæ¥è‡ª GraphQL enrichï¼‰ï¼š
    {
      "full_name": "...",
      "url": "...",
      "description": "...",
      "language": "...",
      "stars": 123,
      "license": "...",
      ...
    }
    
    è¾“å‡º tools æ ¼å¼ï¼š
    {
      "name": "...",
      "one_line_profile": "...",
      "detailed_description": "...",
      "domains": ["B1", "B1-01"],
      "subtask_category": [...],
      "application_level": "solver",
      "primary_language": "Python",
      "repo_url": "...",
      "help_website": [...],
      "license": "...",
      "tags": [...]
    }
    """
    if not cfg.gemini_api_key:
        logger.warning("LLM enrichment æœªå¯ç”¨ | åŸå› =ç¼ºå°‘ GEMINI_API_KEY")
        return []
    
    logger.info(f"å¼€å§‹ LLM æ‰¹é‡enrichment | å€™é€‰æ€»æ•°={len(candidates)}, batch_size={batch_size}")
    tools: List[dict] = []
    
    # åˆ†æ‰¹å¤„ç†ï¼Œé¿å…å•æ¬¡ prompt è¿‡é•¿
    for i in range(0, len(candidates), batch_size):
        batch = candidates[i : i + batch_size]
        batch_num = i // batch_size + 1
        total_batches = (len(candidates) + batch_size - 1) // batch_size
        logger.info(f"å¤„ç†æ‰¹æ¬¡ {batch_num}/{total_batches} | å€™é€‰={i+1}-{min(i+batch_size, len(candidates))}/{len(candidates)}")
        
        batch_result = _enrich_batch(cfg, cluster=cluster, unit=unit, batch=batch)
        tools.extend(batch_result)
    
    return tools


def _enrich_batch(
    cfg: EnumConfig,
    *,
    cluster: LeafCluster,
    unit: Unit,
    batch: List[dict],
    max_parse_retries: int = 3,
) -> List[dict]:
    """å¤„ç†å•æ‰¹å€™é€‰ï¼Œæ”¯æŒ JSON è§£æå¤±è´¥æ—¶çš„é‡è¯•"""
    
    # æ„å»ºå€™é€‰åˆ—è¡¨çš„ç®€è¦ä¿¡æ¯
    candidates_info = []
    for idx, cand in enumerate(batch):
        candidates_info.append({
            "idx": idx,
            "full_name": cand.get("full_name"),
            "url": cand.get("url"),
            "description": cand.get("description"),
            "language": cand.get("language"),
            "stars": cand.get("stars"),
            "license": cand.get("license"),
        })
    
    prompt = f"""ä½ æ˜¯ AI4Sï¼ˆAI for Scienceï¼‰ç§‘ç ”å·¥å…·ç”Ÿæ€ä¸“å®¶ã€‚

**ä»»åŠ¡**ï¼šä¸¥æ ¼åˆ¤æ–­ä»¥ä¸‹å€™é€‰æ˜¯å¦æ˜¯ã€Œç§‘ç ”å·¥å…·ã€ï¼Œå¹¶ç”Ÿæˆå®Œæ•´å®šä¹‰ã€‚

**å¶å­ç°‡ä¸Šä¸‹æ–‡**ï¼š
- ç°‡ID: {cluster.leaf_cluster_id} - {cluster.leaf_cluster_name}
- é¢†åŸŸ: {cluster.domain}
- å…¸å‹å¯¹è±¡: {cluster.typical_objects}
- å•å…ƒID: {unit.unit_id} - {unit.unit_name}
- è¦†ç›–èŒƒå›´: {unit.coverage_tools}

**å€™é€‰åˆ—è¡¨**ï¼š
```json
{json.dumps(candidates_info, ensure_ascii=False, indent=2)}
```

---

## âš ï¸ ä¸‰æ¡ç¡¬è¿‡æ»¤è§„åˆ™ï¼ˆä¸æ»¡è¶³ä»»ä¸€æ¡ â†’ is_tool=falseï¼‰

### ğŸ§± è§„åˆ™1ï¼šç§‘å­¦ä»»åŠ¡å¯æ˜ å°„æ€§ï¼ˆæœ€é‡è¦ï¼‰
å·¥å…·å¿…é¡»èƒ½ç›´æ¥ç”¨äºä»¥ä¸‹è‡³å°‘ä¸€ç§ç§‘å­¦ä»»åŠ¡ï¼š
- âœ… ç§‘å­¦æ•°æ®ç”Ÿæˆï¼ˆsimulation/synthesisï¼‰
- âœ… ç§‘å­¦æ•°æ®å¤„ç†ï¼ˆalignment/filtering/QC/normalizationï¼‰
- âœ… ç§‘å­¦æ•°æ®åˆ†æï¼ˆinference/estimation/statisticsï¼‰
- âœ… ç§‘å­¦å»ºæ¨¡ï¼ˆphysics/chemistry/biology/materialsï¼‰
- âœ… ç§‘å­¦æ¨æ–­ï¼ˆstructure prediction/dynamics/interactionï¼‰
- âœ… ç§‘å­¦å¯è§†åŒ–ï¼ˆä¸“ç”¨äºç§‘å­¦æ•°æ®ï¼‰

**å…¸å‹åä¾‹ï¼ˆç›´æ¥æ’é™¤ï¼‰**ï¼š
- âŒ é€šç”¨ç¼–ç¨‹åº“ï¼ˆnumpy/pandas é™¤å¤–ï¼‰
- âŒ Webæ¡†æ¶ï¼ˆFastAPI/Flaskç­‰ï¼‰
- âŒ å‰ç«¯ç»„ä»¶ï¼ˆReact/Vueç­‰ï¼‰
- âŒ DevOps/CI/CDå·¥å…·

### ğŸ§± è§„åˆ™2ï¼šæ’é™¤"è½¯ä»¶å·¥ç¨‹å·¥å…·"
å¦‚æœå·¥å…·çš„ä¸»è¦ä»·å€¼æ˜¯ä»¥ä¸‹ä¹‹ä¸€ï¼Œç›´æ¥æ’é™¤ï¼š
- âŒ Web API æ¡†æ¶/REST æœåŠ¡
- âŒ App UI/å‰ç«¯ç»„ä»¶
- âŒ å¤šåª’ä½“ç¼–è¾‘/æ’­æ”¾å™¨
- âŒ å®‰å…¨æ‰«æ/SAST/æµ‹è¯•æ¡†æ¶
- âŒ DevOps/éƒ¨ç½²/ç›‘æ§

### ğŸ§± è§„åˆ™3ï¼šæ’é™¤"éå·¥å…·å‹ä»“åº“"
- âŒ Papers list / Awesome list / Curated resources
- âŒ æ•™ç¨‹/ç¬”è®°/è¯¾ç¨‹ä½œä¸š
- âŒ çº¯æ–‡æ¡£/README å‹ä»“åº“
- âŒ è®ºæ–‡å¤ç°ä½†æ— å·¥å…·åŒ–æ¥å£
- âŒ ä¸ªäººå­¦ä¹ é¡¹ç›®ï¼ˆstar < 20 ä¸”æ— ç§‘å­¦æœºæ„èƒŒä¹¦ï¼‰

---

## âœ… åˆæ ¼çš„ç§‘ç ”å·¥å…·ç¤ºä¾‹ï¼ˆå¯¹æ ‡ï¼‰
- FastQC, Trimmomatic, Cutadaptï¼ˆç”Ÿä¿¡QCï¼‰
- BWA, STAR, Bowtieï¼ˆåºåˆ—æ¯”å¯¹ï¼‰
- GATK, Picardï¼ˆå˜å¼‚æ£€æµ‹ï¼‰
- STalign, ClipKIT, PhyKITï¼ˆè¿›åŒ–/ç³»ç»Ÿå‘è‚²ï¼‰
- AlphaFold, RoseTTAFoldï¼ˆç»“æ„é¢„æµ‹ï¼‰

---

## è¾“å‡ºæ ¼å¼

**ä¸¥æ ¼è¾“å‡º JSON å¯¹è±¡ï¼ˆå¿…é¡»åŒ…å« "results" å­—æ®µï¼Œå€¼ä¸ºæ•°ç»„ï¼Œä¸è¦é¢å¤–æ–‡å­—ï¼‰**ï¼š
```json
{{
  "results": [
    {{
      "idx": 0,
      "is_tool": true,
      "name": "FastQC",
      "one_line_profile": "Quality control tool for high throughput sequence data",
      "detailed_description": "...",
      "domains": ["{cluster.leaf_cluster_id}", "{unit.unit_id}"],
      "subtask_category": ["quality_control", "qc_report"],
      "application_level": "solver",
      "primary_language": "Java",
      "repo_url": "...",
      "help_website": [...],
      "license": "GPL-3.0",
      "tags": ["fastq", "quality-control", "ngs"]
    }},
    {{
      "idx": 1,
      "is_tool": false,
      "reason": "é€šç”¨ Web æ¡†æ¶ï¼Œä¸ç¬¦åˆè§„åˆ™1ï¼ˆæ— ç§‘å­¦ä»»åŠ¡æ˜ å°„ï¼‰"
    }},
    {{
      "idx": 2,
      "is_tool": false,
      "reason": "Papers listï¼Œä¸ç¬¦åˆè§„åˆ™3ï¼ˆéå·¥å…·å‹ä»“åº“ï¼‰"
    }}
  ]
}}
```

**å­—æ®µè¯´æ˜**ï¼š
- application_level: åªèƒ½æ˜¯ library/solver/workflow/platform/dataset/service
- subtask_category: å¿…é¡»æ˜¯ç§‘å­¦ä»»åŠ¡ç›¸å…³ï¼ˆå¦‚ quality_control, alignment, variant_callingï¼‰
- æ‰€æœ‰æè¿°ç”¨è‹±æ–‡

**å…³é”®**ï¼š
1. å¿…é¡»è¾“å‡ºæœ‰æ•ˆçš„ JSON å¯¹è±¡ï¼ŒåŒ…å« "results" å­—æ®µ
2. "results" å¿…é¡»æ˜¯æ•°ç»„ï¼Œæ¯ä¸ªå€™é€‰éƒ½å¿…é¡»å‡ºç°ï¼ˆé€šè¿‡ idx å¯¹åº”ï¼‰
3. is_tool=false å¿…é¡»ç»™å‡ºæ˜ç¡® reasonï¼ˆå¼•ç”¨ä¸‰æ¡è§„åˆ™ï¼‰
4. ä¸¥æ ¼æ‰§è¡Œä¸‰æ¡ç¡¬è§„åˆ™ï¼Œ**å®å¯æ¼æ‰è¾¹ç¼˜å·¥å…·ï¼Œä¹Ÿä¸è¦æ··å…¥å™ªå£°**
"""

    # ç›´æ¥ä½¿ç”¨ä¸ llm_query_generator.py ç›¸åŒçš„ HTTP è¯·æ±‚æ–¹å¼
    for parse_attempt in range(1, max_parse_retries + 1):
        # logger.info("prompt",prompt)
        resp = safe_request(
            "POST",
            f"{cfg.gemini_api_base}/chat/completions",
            headers={
                "Authorization": f"Bearer {cfg.gemini_api_key}",
                "Content-Type": "application/json",
            },
            json_data={
                "model": cfg.gemini_model,
                "temperature": 0.1,
                "messages": [{"role": "user", "content": prompt}],
            },
            timeout=600,
            max_retries=40,
        )
        # logger.info("resp",resp.json())
        if resp is None:
            if parse_attempt == max_parse_retries:
                logger.warning(f"LLM enrichment HTTP è¯·æ±‚å¤±è´¥ | æ‰¹æ¬¡å¤§å°={len(batch)}, å·²é‡è¯• {max_parse_retries} æ¬¡")
                return []
            logger.warning(f"LLM enrichment HTTP è¯·æ±‚å¤±è´¥ï¼Œé‡è¯•ä¸­ | attempt={parse_attempt}/{max_parse_retries}")
            time.sleep(2 ** parse_attempt)
            continue

        try:
            content = resp.json()["choices"][0]["message"]["content"]
            # å°è¯•æå– JSON
            content = content.strip()
            if content.startswith("```"):
                # å»é™¤ markdown ä»£ç å—
                lines = content.split("\n")
                content = "\n".join(lines[1:-1]) if len(lines) > 2 else content
            
            # è§£æ JSON
            parsed = json.loads(content)
            
            # å¦‚æœç›´æ¥æ˜¯æ•°ç»„ï¼Œä½¿ç”¨å®ƒ
            if isinstance(parsed, list):
                results = parsed
            # å¦‚æœæ˜¯å¯¹è±¡ï¼Œä¼˜å…ˆæŸ¥æ‰¾ "results" å­—æ®µ
            elif isinstance(parsed, dict):
                if "results" in parsed and isinstance(parsed["results"], list):
                    results = parsed["results"]
                else:
                    # æŸ¥æ‰¾ç¬¬ä¸€ä¸ªæ•°ç»„ç±»å‹çš„å€¼
                    results = None
                    for key, value in parsed.items():
                        if isinstance(value, list):
                            results = value
                            break
                    if results is None:
                        logger.warning(
                            f"LLM è¿”å› JSON å¯¹è±¡ä½†æ— æ•°ç»„å­—æ®µ | attempt={parse_attempt}/{max_parse_retries}, "
                            f"keys={list(parsed.keys())}, content_preview={content[:200]}"
                        )
                        if parse_attempt < max_parse_retries:
                            time.sleep(2 ** parse_attempt)
                            continue
                        return []
            else:
                logger.warning(
                    f"LLM è¿”å›å†…å®¹ä¸æ˜¯æ•°ç»„æˆ–å¯¹è±¡ | attempt={parse_attempt}/{max_parse_retries}, "
                    f"type={type(parsed)}, content_preview={content[:200]}"
                )
                if parse_attempt < max_parse_retries:
                    time.sleep(2 ** parse_attempt)
                    continue
                return []
            
            # æå– is_tool=true çš„å·¥å…·
            tools: List[dict] = []
            for item in results:
                if not isinstance(item, dict):
                    continue
                if item.get("is_tool"):
                    tool = {k: v for k, v in item.items() if k != "idx" and k != "is_tool"}
                    tools.append(tool)
            
            logger.debug(f"æ‰¹æ¬¡è§£ææˆåŠŸ | è¯†åˆ«å·¥å…·æ•°={len(tools)}/{len(batch)}")
            return tools
            
        except Exception as e:
            logger.warning(f"LLM enrichment å“åº”è§£æå¤±è´¥ | attempt={parse_attempt}/{max_parse_retries}, error={e}")
            if parse_attempt < max_parse_retries:
                time.sleep(2 ** parse_attempt)
                continue
            return []
    
    return []

